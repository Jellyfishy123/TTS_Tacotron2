### **Contributors:**
Misbah Mohammed (Youtube channel) https://www.youtube.com/watch?v=euBGpUF6YN8&t=459s
<br> Google's Paper on Tacotron : https://arxiv.org/pdf/1712.05884.pdf
<br> NVIDIA GitHub: https://github.com/NVIDIA/tacotron2
<br> LJ Speech Dataset: https://keithito.com/LJ-Speech-Dataset/

<br> Nguyen Tien Dat
<br> The original code is from https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/nvidia_deeplearningexamples_tacotron2.ipynb
<br> The code which I have modified is under file TTS_NguyenTienDat.ipynb

### **About:**
Hey everyone, I've just started interning at A Star Insitute of Infocomm (I2R) and I'm pleased to announce that I'll be dealing with a lot of TTS (Text-to-Speech) models and Singing Voice Synthesis (SVS) models for the next 4 months. This is part of my AI and ML research to synthesis music from independent inputs using appropriate models. This is the very first model I analysed. While it only does TTS by typing what the reader wants the model to say in a cell, I took it a step further. Firstly, I chose a random song on Youtube (SZA- Kill Bill) but only the vocals, as we can only used voice isolation. I converted it into a wav file and fed it into the model. After getting the text from the model, I converted it back into a wav file (speech). However, I found out that the tone and pitch of the singer are lost when passed though this model. Hence, it does not fulfil my SVS research. However, it is a nice model for TTS and I'll be exploring alternatives. Please reach out to me (tiendat311003@gmail.com) for more information. Thank you for reading! 

